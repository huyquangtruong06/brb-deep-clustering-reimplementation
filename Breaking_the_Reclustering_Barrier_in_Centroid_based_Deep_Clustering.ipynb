{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hngwwc_yFwzG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image, ImageFilter\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxNMfsnvEpm1"
      },
      "outputs": [],
      "source": [
        "class TwoCropTransform:\n",
        "    \"\"\"\n",
        "    Wrapper return two views (views) of the same image.\n",
        "    Scenario 3 (Contrastive Learning / SimCLR).\n",
        "    \"\"\"\n",
        "    def __init__(self, base_transform):\n",
        "        self.base_transform = base_transform\n",
        "\n",
        "    def __call__(self, x):\n",
        "        q = self.base_transform(x)\n",
        "        k = self.base_transform(x)\n",
        "        return [q, k]\n",
        "\n",
        "class GaussianBlur(object):\n",
        "    \"\"\"Gaussian Blur augmentation (SimCLR).\"\"\"\n",
        "    def __init__(self, sigma=[.1, 2.]):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def __call__(self, x):\n",
        "        sigma = random.uniform(self.sigma[0], self.sigma[1])\n",
        "        x = x.filter(ImageFilter.GaussianBlur(radius=sigma))\n",
        "        return x\n",
        "\n",
        "def get_brb_transform(dataset_name, is_contrastive=False, train=True):\n",
        "    \"\"\"\n",
        "    - train=True: Apply Augmentation (Affine or SimCLR).\n",
        "    - train=False: Only Normalize (Test/Validation).\n",
        "    \"\"\"\n",
        "\n",
        "    # Group 1: GRAYSCALE\n",
        "    if dataset_name in ['MNIST', 'KMNIST', 'Fashion-MNIST', 'USPS', 'OPTDIGITS']:\n",
        "        if train:\n",
        "            # Use Random Affine for grayscale\n",
        "            transform_list = [\n",
        "                transforms.ToPILImage() if dataset_name == 'OPTDIGITS' else transforms.Lambda(lambda x: x),\n",
        "                transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5,), (0.5,))\n",
        "            ]\n",
        "        else:\n",
        "            transform_list = [\n",
        "                transforms.ToPILImage() if dataset_name == 'OPTDIGITS' else transforms.Lambda(lambda x: x),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5,), (0.5,))\n",
        "            ]\n",
        "\n",
        "        final_transform = transforms.Compose(transform_list)\n",
        "\n",
        "    # Group 2: COLOR (CIFAR-10, CIFAR-100-20, GTSRB)\n",
        "    elif dataset_name in ['CIFAR-10', 'CIFAR-100-20', 'GTSRB']:\n",
        "        # Mean/Std of CIFAR/ImageNet\n",
        "        normalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
        "        input_size = 32\n",
        "\n",
        "        if train:\n",
        "            # Use SimCLR augmentations for data color.\n",
        "            final_transform = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(size=input_size, scale=(0.2, 1.)),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomApply([\n",
        "                    transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n",
        "                ], p=0.8),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.ToTensor(),\n",
        "                normalize,\n",
        "            ])\n",
        "        else:\n",
        "            # Resize to Std vÃ  Normalize\n",
        "            final_transform = transforms.Compose([\n",
        "                transforms.Resize((input_size, input_size)) if dataset_name == 'GTSRB' else transforms.Lambda(lambda x: x),\n",
        "                transforms.ToTensor(),\n",
        "                normalize,\n",
        "            ])\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
        "\n",
        "    if is_contrastive and train:\n",
        "        return TwoCropTransform(final_transform)\n",
        "\n",
        "    return final_transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thP6B1rvGysj"
      },
      "outputs": [],
      "source": [
        "class OptDigitsDataset(Dataset):\n",
        "    def __init__(self, data, targets, transform=None):\n",
        "        self.data = torch.tensor(data, dtype=torch.float32).view(-1, 8, 8)\n",
        "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.data[idx], self.targets[idx]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "class GTSRBDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.data.iloc[idx]['Path'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = int(self.data.iloc[idx]['ClassId'])\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "class CIFAR100Coarse(Dataset):\n",
        "    \"\"\"\n",
        "    Wrapper mapping CIFAR-100 (100 classes) -> CIFAR-100-20 (20 superclasses).\n",
        "    \"\"\"\n",
        "    def __init__(self, root, train, transform, download):\n",
        "        self.dataset = datasets.CIFAR100(root=root, train=train, transform=transform, download=download)\n",
        "\n",
        "        # Hardcoded mapping from Fine (0-99) to Coarse (0-19)\n",
        "        self.coarse_map = np.array([\n",
        "            4, 1, 14, 8, 0, 6, 7, 7, 18, 3, 3, 14, 9, 18, 7, 11, 3, 9, 7, 11,\n",
        "            6, 11, 5, 10, 7, 6, 13, 15, 3, 15, 0, 11, 1, 10, 12, 14, 16, 9, 11, 5,\n",
        "            5, 19, 8, 8, 15, 13, 14, 17, 18, 10, 16, 4, 17, 4, 2, 0, 17, 4, 18, 17,\n",
        "            10, 3, 2, 12, 12, 16, 12, 1, 9, 19, 2, 10, 0, 1, 16, 12, 9, 13, 15, 13,\n",
        "            16, 19, 2, 4, 6, 19, 5, 5, 8, 19, 18, 1, 2, 15, 6, 0, 17, 8, 14, 13\n",
        "        ])\n",
        "\n",
        "        # Pre-calculate coarse labels\n",
        "        self.targets = [self.coarse_map[y] for y in self.dataset.targets]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # self.dataset[idx] applied transform\n",
        "        img, _ = self.dataset[idx]\n",
        "        return img, self.targets[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbBZUNe3G3-J"
      },
      "outputs": [],
      "source": [
        "def get_data_loaders(dataset_name,\n",
        "                     batch_size=32,\n",
        "                     download_dir='./datasets',\n",
        "                     transform=None,\n",
        "                     is_contrastive=False):\n",
        "    \"\"\"\n",
        "    load data\n",
        "\n",
        "    Args:\n",
        "        dataset_name (str): name (MNIST, CIFAR-10, CIFAR-100-20, ...).\n",
        "        batch_size (int): size batch.\n",
        "        download_dir (str): path data.\n",
        "        transform (transforms): (Optional) Define your own transform if you don't want to use the default one.\n",
        "        is_contrastive (bool): If True, enable TwoCropTransform mode for SimCLR (Scenario 3).\n",
        "\n",
        "    Returns:\n",
        "        train_loader, test_loader\n",
        "    \"\"\"\n",
        "    os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "    # Transform Strategy\n",
        "    if transform is None:\n",
        "        # Train: Apply Augmentation (Affine or Color Jitter) + TwoCrop (if contrastive)\n",
        "        train_transform = get_brb_transform(dataset_name, is_contrastive=is_contrastive, train=True)\n",
        "        test_transform = get_brb_transform(dataset_name, is_contrastive=False, train=False)\n",
        "    else:\n",
        "        train_transform = transform\n",
        "        test_transform = transform\n",
        "\n",
        "    # Init Dataset\n",
        "    if dataset_name == 'MNIST':\n",
        "        train_dataset = datasets.MNIST(download_dir, True, download=True, transform=train_transform)\n",
        "        test_dataset  = datasets.MNIST(download_dir, False, download=True, transform=test_transform)\n",
        "\n",
        "    elif dataset_name == 'KMNIST':\n",
        "        train_dataset = datasets.KMNIST(download_dir, True, download=True, transform=train_transform)\n",
        "        test_dataset  = datasets.KMNIST(download_dir, False, download=True, transform=test_transform)\n",
        "\n",
        "    elif dataset_name == 'Fashion-MNIST':\n",
        "        train_dataset = datasets.FashionMNIST(download_dir, True, download=True, transform=train_transform)\n",
        "        test_dataset  = datasets.FashionMNIST(download_dir, False, download=True, transform=test_transform)\n",
        "\n",
        "    elif dataset_name == 'USPS':\n",
        "        train_dataset = datasets.USPS(download_dir, True, download=True, transform=train_transform)\n",
        "        test_dataset  = datasets.USPS(download_dir, False, download=True, transform=test_transform)\n",
        "\n",
        "    elif dataset_name == 'OPTDIGITS':\n",
        "        digits = load_digits()\n",
        "        X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42)\n",
        "        train_dataset = OptDigitsDataset(X_train, y_train, train_transform)\n",
        "        test_dataset  = OptDigitsDataset(X_test, y_test, test_transform)\n",
        "\n",
        "    elif dataset_name == 'CIFAR-10':\n",
        "        train_dataset = datasets.CIFAR10(download_dir, True, download=True, transform=train_transform)\n",
        "        test_dataset  = datasets.CIFAR10(download_dir, False, download=True, transform=test_transform)\n",
        "\n",
        "    elif dataset_name == 'CIFAR-100-20':\n",
        "        train_dataset = CIFAR100Coarse(download_dir, True, train_transform, True)\n",
        "        test_dataset  = CIFAR100Coarse(download_dir, False, test_transform, True)\n",
        "\n",
        "    elif dataset_name == 'GTSRB':\n",
        "        dataset_path = os.path.join(download_dir, 'gtsrb')\n",
        "        if not os.path.exists(os.path.join(dataset_path, 'Train.csv')):\n",
        "            print(\"Downloading GTSRB from Kaggle...\")\n",
        "            try:\n",
        "                os.system(f'kaggle datasets download -d meowmeowmeowmeowmeow/gtsrb-german-traffic-sign -p {download_dir}')\n",
        "                with zipfile.ZipFile(os.path.join(download_dir, 'gtsrb-german-traffic-sign.zip'), 'r') as zip_ref:\n",
        "                    zip_ref.extractall(dataset_path)\n",
        "            except Exception as e:\n",
        "                print(f\"Error downloading GTSRB: {e}. Please manually download or setup Kaggle API.\")\n",
        "\n",
        "        train_dataset = GTSRBDataset(os.path.join(dataset_path, 'Train.csv'), dataset_path, train_transform)\n",
        "        test_dataset = GTSRBDataset(os.path.join(dataset_path, 'Test.csv'), dataset_path, test_transform)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Dataset '{dataset_name}' is not supported in BRB project.\")\n",
        "\n",
        "    # Create DataLoader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=2)\n",
        "    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVx9qtSBK7Qt"
      },
      "source": [
        "**Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6U9P2xHKqNv"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "def init_weights(m):\n",
        "    \"\"\"\n",
        "    Kaiming Uniform Initialization.\n",
        "    Paper BRB yÃªu cáº§u sá»­ dá»¥ng Kaiming init khi reset weights[cite: 206].\n",
        "    \"\"\"\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.fill_(0.0)\n",
        "\n",
        "\n",
        "# FEED-FORWARD AUTOENCODER (Group 1: ['MNIST', 'KMNIST', 'Fashion-MNIST', 'USPS', 'OPTDIGITS', etc.)\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim=784, latent_dim=10, dims=[500, 500, 2000]):\n",
        "        \"\"\"\n",
        "        Kiáº¿n trÃºc chuáº©n cá»§a DEC/IDEC Ä‘Æ°á»£c dÃ¹ng trong BRB.\n",
        "        Máº·c Ä‘á»‹nh cho MNIST: 784 -> 500 -> 500 -> 2000 -> 10\n",
        "        \"\"\"\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder_layers = nn.ModuleList()\n",
        "        curr_dim = input_dim\n",
        "\n",
        "        # hidden layer(500, 500, 2000)\n",
        "        for dim in dims:\n",
        "            self.encoder_layers.append(nn.Linear(curr_dim, dim))\n",
        "            self.encoder_layers.append(nn.ReLU())\n",
        "            curr_dim = dim\n",
        "\n",
        "        # layer Latent (Embedding)\n",
        "        self.embedding_layer = nn.Linear(curr_dim, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder_layers = nn.ModuleList()\n",
        "        # Reverse : 2000 -> 500 -> 500\n",
        "        reversed_dims = list(reversed(dims))\n",
        "\n",
        "        # from latent\n",
        "        self.decoder_layers.append(nn.Linear(latent_dim, reversed_dims[0]))\n",
        "        self.decoder_layers.append(nn.ReLU())\n",
        "        curr_dim = reversed_dims[0]\n",
        "\n",
        "        # hidden between\n",
        "        for i in range(1, len(reversed_dims)):\n",
        "            self.decoder_layers.append(nn.Linear(curr_dim, reversed_dims[i]))\n",
        "            self.decoder_layers.append(nn.ReLU())\n",
        "            curr_dim = reversed_dims[i]\n",
        "\n",
        "        # Reconstruction\n",
        "        self.reconst_layer = nn.Linear(curr_dim, input_dim)\n",
        "\n",
        "        # init weight\n",
        "        self.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() > 2:\n",
        "            x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Encode\n",
        "        h = x\n",
        "        for layer in self.encoder_layers:\n",
        "            h = layer(h)\n",
        "        z = self.embedding_layer(h)\n",
        "\n",
        "        # Decode\n",
        "        r = z\n",
        "        for layer in self.decoder_layers:\n",
        "            r = layer(r)\n",
        "        x_recon = self.reconst_layer(r)\n",
        "\n",
        "        # use Tanh\n",
        "        x_recon = torch.tanh(x_recon)\n",
        "\n",
        "        return x_recon, z\n",
        "\n",
        "# RESNET-18 BACKBONE (Group 2: CIFAR-10, GTSRB)\n",
        "\n",
        "class ContrastiveResNet18(nn.Module):\n",
        "    def __init__(self, latent_dim=128):\n",
        "        \"\"\"\n",
        "        ResNet-18 backbone modified for CIFAR small images + Projection Head.\n",
        "        \"\"\"\n",
        "        super(ContrastiveResNet18, self).__init__()\n",
        "\n",
        "        # Load standard ResNet18\n",
        "        backbone = resnet18(pretrained=False)\n",
        "\n",
        "        # MODIFICATION CHO CIFAR/GTSRB (32x32 images)\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            backbone.bn1,\n",
        "            backbone.relu,\n",
        "            nn.Identity(),\n",
        "            backbone.layer1,\n",
        "            backbone.layer2,\n",
        "            backbone.layer3,\n",
        "            backbone.layer4,\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        self.feature_dim = 512\n",
        "        self.projector = nn.Sequential(\n",
        "            nn.Linear(self.feature_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, latent_dim)\n",
        "        )\n",
        "\n",
        "        self.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Feature Extraction (Representation h)\n",
        "        h = self.features(x)\n",
        "        h = h.view(h.size(0), -1) # Flatten (batch, 512)\n",
        "\n",
        "        # Projection (z)\n",
        "        z = self.projector(h)\n",
        "\n",
        "        # Return both h (for clustering) vÃ  z (for training loss SimCLR)\n",
        "        return h, z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x04-enx7MQ27"
      },
      "source": [
        "**metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsdAQfk5MdAB"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "def cluster_acc(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        y_true (np.array): Ground Truth\n",
        "        y_pred (np.array): Predicted Clusters\n",
        "\n",
        "    Returns:\n",
        "        float: Accuracy [0, 1].\n",
        "    \"\"\"\n",
        "    y_true = y_true.astype(np.int64)\n",
        "    y_pred = y_pred.astype(np.int64)\n",
        "\n",
        "    # Size of  y_pred and y_true must same\n",
        "    assert y_pred.size == y_true.size\n",
        "\n",
        "    D = max(y_pred.max(), y_true.max()) + 1\n",
        "    w = np.zeros((D, D), dtype=np.int64)\n",
        "\n",
        "    # Confusion Matrix\n",
        "    # w[i, j] = number of samples belonging to cluster i but with actual label j\n",
        "    for i in range(y_pred.size):\n",
        "        w[y_pred[i], y_true[i]] += 1\n",
        "\n",
        "    row_ind, col_ind = linear_sum_assignment(w.max() - w)\n",
        "    accuracy = sum([w[i, j] for i, j in zip(row_ind, col_ind)]) * 1.0 / y_pred.size\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "def evaluate_clustering(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate all the key metrics for Deep Clustering.\n",
        "\n",
        "    Args:\n",
        "        y_true (np.array): Ground Truth\n",
        "        y_pred (np.array): Predicted Clusters\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary contain {'ACC': float, 'NMI': float, 'ARI': float}\n",
        "    \"\"\"\n",
        "    if hasattr(y_true, 'cpu'): y_true = y_true.cpu().numpy()\n",
        "    if hasattr(y_pred, 'cpu'): y_pred = y_pred.cpu().numpy()\n",
        "    if isinstance(y_true, list): y_true = np.array(y_true)\n",
        "    if isinstance(y_pred, list): y_pred = np.array(y_pred)\n",
        "\n",
        "    # Accuracy (ACC)\n",
        "    acc = cluster_acc(y_true, y_pred)\n",
        "\n",
        "    # Normalized Mutual Information (NMI)\n",
        "    nmi = normalized_mutual_info_score(y_true, y_pred)\n",
        "\n",
        "    # Adjusted Rand Index (ARI)\n",
        "    ari = adjusted_rand_score(y_true, y_pred)\n",
        "\n",
        "    return {\n",
        "        'ACC': np.round(acc, 4),\n",
        "        'NMI': np.round(nmi, 4),\n",
        "        'ARI': np.round(ari, 4)\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8itrwTUPGPW"
      },
      "source": [
        "**dec**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9A0tdJgRPQzU"
      },
      "outputs": [],
      "source": [
        "class ClusteringLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    The Clustering Layer stores the Centroids and calculates the Q distribution.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters, n_z, alpha=1.0):\n",
        "        super(ClusteringLayer, self).__init__()\n",
        "        self.n_clusters = n_clusters\n",
        "        self.n_z = n_z\n",
        "        self.alpha = alpha # Alpha=1.0 -  Student's t-distribution\n",
        "\n",
        "        # Centroids is the training parameter.(nn.Parameter)\n",
        "        # Shape: (number of cluster, size latent)\n",
        "        self.centroids = nn.Parameter(torch.Tensor(n_clusters, n_z))\n",
        "\n",
        "        # init centroids\n",
        "        nn.init.xavier_normal_(self.centroids.data)\n",
        "\n",
        "    def forward(self, z):\n",
        "        \"\"\"\n",
        "        Compute Soft Assignment (Q) :  latent z and centroids.\n",
        "        Input: z (batch_size, n_z)\n",
        "        Output: q (batch_size, n_clusters)\n",
        "        \"\"\"\n",
        "        diff = z.unsqueeze(1) - self.centroids.unsqueeze(0)\n",
        "        squared_dist = torch.sum(diff**2, dim=2) # (batch, n_clusters)\n",
        "\n",
        "        q = 1.0 + (squared_dist / self.alpha)\n",
        "        q = torch.pow(q, -(self.alpha + 1.0) / 2.0)\n",
        "\n",
        "        # Normalize\n",
        "        q = torch.div(q, torch.sum(q, dim=1, keepdim=True))\n",
        "\n",
        "        return q\n",
        "\n",
        "def target_distribution(q):\n",
        "    weight = q**2 / q.sum(0)\n",
        "    p = (weight.t() / weight.sum(1)).t()\n",
        "    return p.detach()\n",
        "\n",
        "def kl_divergence_loss(q, p):\n",
        "    \"\"\"\n",
        "    KL Divergence Loss: KL(P || Q) = sum(p * log(p/q))\n",
        "    \"\"\"\n",
        "    return F.kl_div(torch.log(q), p, reduction='batchmean')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT7iICZ7Qfmz"
      },
      "source": [
        "**idec**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmitP2AXQm_3"
      },
      "outputs": [],
      "source": [
        "class IDEC(nn.Module):\n",
        "    \"\"\"\n",
        "    Improved Deep Embedded Clustering (IDEC).\n",
        "    combine Autoencoder + Clustering Layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, autoencoder, n_clusters, alpha=1.0):\n",
        "        super(IDEC, self).__init__()\n",
        "        self.autoencoder = autoencoder\n",
        "        self.n_clusters = n_clusters\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.latent_dim = autoencoder.embedding_layer.out_features\n",
        "\n",
        "        # init Clustering Layer (similar DEC)\n",
        "        self.clustering_layer = ClusteringLayer(self.n_clusters, self.latent_dim, self.alpha)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        Input: x\n",
        "        Output:\n",
        "            - x_recon: Reconstructed image (for calculating L_rec)\n",
        "            - q: Soft distribution (for calculating L_clus)\n",
        "            - z: Latent vector\n",
        "        \"\"\"\n",
        "        # Autoencoder\n",
        "        x_recon, z = self.autoencoder(x)\n",
        "\n",
        "        # Clustering Layer\n",
        "        q = self.clustering_layer(z)\n",
        "\n",
        "        return x_recon, q, z\n",
        "\n",
        "def idec_loss_function(x, x_recon, q, p, gamma=0.1):\n",
        "    \"\"\"\n",
        "    IDEC: L = L_rec + gamma * L_clus\n",
        "\n",
        "    Args:\n",
        "        x: origin img\n",
        "        x_recon: Reconstructed image from Decoder.\n",
        "        q: Soft assignment from Clustering Layer.\n",
        "        p: Target distribution.\n",
        "        gamma: hyperparameter.\n",
        "\n",
        "    Returns:\n",
        "        total_loss, reconstruction_loss, clustering_loss\n",
        "    \"\"\"\n",
        "    # Reconstruction Loss (MSE)\n",
        "    if x.dim() > 2:\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "    loss_rec = F.mse_loss(x_recon, x)\n",
        "\n",
        "    # Clustering Loss (KL Divergence)\n",
        "    loss_clus = kl_divergence_loss(q, p)\n",
        "\n",
        "    # Total Loss\n",
        "    total_loss = loss_rec + gamma * loss_clus\n",
        "\n",
        "    return total_loss, loss_rec, loss_clus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8o4w4GzR1Gr"
      },
      "source": [
        "**dcn**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOmQzN0vSa-8"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "class DCN(nn.Module):\n",
        "    \"\"\"\n",
        "    Deep Clustering Network (DCN).\n",
        "    Alternating Optimization:\n",
        "    1. K-Means to update Centroids & Assignments (Labels).\n",
        "    2. SGD to update Autoencoder weights.\n",
        "    \"\"\"\n",
        "    def __init__(self, autoencoder, n_clusters):\n",
        "        super(DCN, self).__init__()\n",
        "        self.autoencoder = autoencoder\n",
        "        self.n_clusters = n_clusters\n",
        "        self.latent_dim = autoencoder.embedding_layer.out_features\n",
        "\n",
        "        self.register_buffer('centroids', torch.zeros(n_clusters, self.latent_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of DCN : cháº¡y Autoencoder.\n",
        "        \"\"\"\n",
        "        x_recon, z = self.autoencoder(x)\n",
        "        return x_recon, z\n",
        "\n",
        "    def update_centroids(self, z_full_data):\n",
        "        \"\"\"\n",
        "        Step 1: Freeze Embedding -> Update Centroids (use K-Means).\n",
        "        All dataset.\n",
        "\n",
        "        Args:\n",
        "            z_full_data (np.array): Latent vectors of train.\n",
        "\n",
        "        Returns:\n",
        "            assignments (np.array): hard label.\n",
        "        \"\"\"\n",
        "        # Hard Clustering\n",
        "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20, random_state=42)\n",
        "        assignments = kmeans.fit_predict(z_full_data)\n",
        "\n",
        "        # upadte new centroids\n",
        "        new_centroids = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32)\n",
        "\n",
        "        # assign buffer\n",
        "        self.centroids.copy_(new_centroids)\n",
        "\n",
        "        return assignments\n",
        "\n",
        "def dcn_loss_function(x, x_recon, z, assignments, centroids, beta=1.0):\n",
        "    \"\"\"\n",
        "    L = L_rec + beta * L_clus\n",
        "    \"\"\"\n",
        "    # Reconstruction Loss\n",
        "    if x.dim() > 2:\n",
        "        x = x.view(x.size(0), -1)\n",
        "    loss_rec = F.mse_loss(x_recon, x)\n",
        "\n",
        "    # Clustering Loss (Hard Assignment Loss)\n",
        "    batch_centroids = centroids[assignments]\n",
        "\n",
        "    loss_clus = F.mse_loss(z, batch_centroids)\n",
        "\n",
        "    # summary\n",
        "    total_loss = loss_rec + beta * loss_clus\n",
        "\n",
        "    return total_loss, loss_rec, loss_clus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkiPZ2ZUTiky"
      },
      "source": [
        "**simclr**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ieL_EHwTkiO"
      },
      "outputs": [],
      "source": [
        "class NTXentLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Normalized Temperature-scaled Cross Entropy Loss cho SimCLR.\n",
        "    Source: Chen et al., 2020.\n",
        "    \"\"\"\n",
        "    def __init__(self, batch_size, temperature=0.5, device='cuda'):\n",
        "        super(NTXentLoss, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.temperature = temperature\n",
        "        self.device = device\n",
        "\n",
        "        self.mask = self.mask_correlated_samples(batch_size)\n",
        "\n",
        "        # Criterion is CrossEntropy\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "        self.similarity_f = nn.CosineSimilarity(dim=2)\n",
        "\n",
        "    def mask_correlated_samples(self, batch_size):\n",
        "        N = 2 * batch_size\n",
        "        mask = torch.ones((N, N), dtype=bool)\n",
        "        mask = mask.fill_diagonal_(0)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            mask[i, batch_size + i] = 0\n",
        "            mask[batch_size + i, i] = 0\n",
        "        return mask\n",
        "\n",
        "    def forward(self, z_i, z_j):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            z_i: Batch vector projection of view 1 (N, dim)\n",
        "            z_j: Batch vector projection of view 2 (N, dim)\n",
        "        \"\"\"\n",
        "        N = 2 * self.batch_size\n",
        "\n",
        "        # (2N, dim)\n",
        "        z = torch.cat((z_i, z_j), dim=0)\n",
        "\n",
        "        # Cosine Similarity Matrix (2N, 2N)\n",
        "        sim = self.similarity_f(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n",
        "\n",
        "        sim_i_j = torch.diag(sim, self.batch_size)\n",
        "        sim_j_i = torch.diag(sim, -self.batch_size)\n",
        "\n",
        "        # Positive logits: (2N, 1)\n",
        "        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)\n",
        "\n",
        "        mask = self.mask.to(self.device)\n",
        "        negative_samples = sim[mask].reshape(N, -1)\n",
        "\n",
        "        # Cross Entropy\n",
        "        labels = torch.zeros(N).to(self.device).long()\n",
        "        logits = torch.cat((positive_samples, negative_samples), dim=1)\n",
        "\n",
        "        loss = self.criterion(logits, labels)\n",
        "        return loss / N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3u5S2idBT9Pp"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "class SelfLabeling:\n",
        "    \"\"\"\n",
        "    Self-Labeling:\n",
        "    1. Generate Pseudo-Labels from currently model (use K-Means).\n",
        "    2. Fine-tune\n",
        "    \"\"\"\n",
        "    def __init__(self, model, train_loader, n_clusters, device='cuda'):\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.n_clusters = n_clusters\n",
        "        self.device = device\n",
        "\n",
        "    def get_pseudo_labels(self):\n",
        "        \"\"\"\n",
        "        get feature and run K-Means.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        features_list = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(self.train_loader):\n",
        "                imgs = batch[0]\n",
        "                if isinstance(imgs, list): # if TwoCropTransform\n",
        "                    imgs = imgs[0]\n",
        "\n",
        "                imgs = imgs.to(self.device)\n",
        "\n",
        "                # Forward cross backbone (ResNet) get representation h\n",
        "                h, _ = self.model(imgs)\n",
        "                features_list.append(h.cpu().numpy())\n",
        "\n",
        "        full_features = np.concatenate(features_list, axis=0)\n",
        "\n",
        "        # Clustering K-Means\n",
        "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20, random_state=42)\n",
        "        pseudo_labels = kmeans.fit_predict(full_features)\n",
        "\n",
        "        return torch.LongTensor(pseudo_labels).to(self.device)\n",
        "\n",
        "    def finetune(self, epochs=20, lr=0.01):\n",
        "        \"\"\"\n",
        "        train classification model base on Pseudo-Labels.\n",
        "        \"\"\"\n",
        "        # get fake label\n",
        "        pseudo_labels = self.get_pseudo_labels()\n",
        "\n",
        "        # Input dim = model.feature_dim\n",
        "        # Output dim = n_clusters\n",
        "        classifier_head = nn.Linear(self.model.feature_dim, self.n_clusters).to(self.device)\n",
        "\n",
        "        # Optimizer for classifier\n",
        "        optimizer = optim.SGD(\n",
        "            list(self.model.parameters()) + list(classifier_head.parameters()),\n",
        "            lr=lr, momentum=0.9, weight_decay=1e-4\n",
        "        )\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "        self.model.train()\n",
        "        classifier_head.train()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "\n",
        "            current_idx = 0\n",
        "            for batch in self.train_loader:\n",
        "                imgs = batch[0]\n",
        "                if isinstance(imgs, list): imgs = imgs[0]\n",
        "                imgs = imgs.to(self.device)\n",
        "\n",
        "                batch_size = imgs.size(0)\n",
        "                targets = pseudo_labels[current_idx : current_idx + batch_size]\n",
        "                current_idx += batch_size\n",
        "\n",
        "                # Forward\n",
        "                h, _ = self.model(imgs) # get feature\n",
        "                logits = classifier_head(h) # Classify\n",
        "\n",
        "                loss = criterion(logits, targets)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - Self-Label Loss: {total_loss / len(self.train_loader):.4f}\")\n",
        "\n",
        "        return self.model, classifier_head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8JU43hIVHdD"
      },
      "source": [
        "**Benchmark**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "\n",
        "class Visualizer:\n",
        "    @staticmethod\n",
        "    def align_labels(y_true, y_pred):\n",
        "        D = max(y_pred.max(), y_true.max()) + 1\n",
        "        w = np.zeros((D, D), dtype=np.int64)\n",
        "        for i in range(y_pred.size):\n",
        "            w[y_pred[i], y_true[i]] += 1\n",
        "        row_ind, col_ind = linear_sum_assignment(w.max() - w)\n",
        "        remapping = {i: j for i, j in zip(row_ind, col_ind)}\n",
        "        return np.array([remapping.get(x, x) for x in y_pred])\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_all(results, y_true, y_pred, features, dataset_name, method_name):\n",
        "        sns.set_theme(style=\"whitegrid\")\n",
        "        fig = plt.figure(figsize=(20, 12))\n",
        "        gs = fig.add_gridspec(2, 2)\n",
        "\n",
        "        # common title\n",
        "        fig.suptitle(f\"{method_name} on {dataset_name}\", fontsize=18, fontweight='bold')\n",
        "\n",
        "        # Chart 1: Metrics\n",
        "        ax1 = fig.add_subplot(gs[0, 0])\n",
        "        sns.barplot(x=list(results.keys()), y=list(results.values()), palette='viridis', ax=ax1)\n",
        "        for i, v in enumerate(results.values()):\n",
        "            ax1.text(i, v + 0.01, f\"{v:.4f}\", ha='center', fontweight='bold')\n",
        "        ax1.set_title(\"Clustering Metrics\", fontsize=14)\n",
        "        ax1.set_ylim(0, 1.1)\n",
        "\n",
        "        # Chart 2: Confusion Matrix\n",
        "        ax2 = fig.add_subplot(gs[0, 1])\n",
        "        y_pred_aligned = Visualizer.align_labels(y_true, y_pred)\n",
        "        cm = confusion_matrix(y_true, y_pred_aligned)\n",
        "        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap='Blues', ax=ax2)\n",
        "        ax2.set_title(\"Confusion Matrix (Aligned)\", fontsize=14)\n",
        "\n",
        "        # Chart 3: t-SNE\n",
        "        ax3 = fig.add_subplot(gs[1, :])\n",
        "        if len(features) > 2000:\n",
        "            idx = np.random.choice(len(features), 2000, replace=False)\n",
        "            feat_sample, y_sample = features[idx], y_true[idx]\n",
        "        else:\n",
        "            feat_sample, y_sample = features, y_true\n",
        "\n",
        "        print(\"   >> Computing t-SNE...\")\n",
        "        tsne = TSNE(n_components=2, init='pca', learning_rate='auto', random_state=42)\n",
        "        z_emb = tsne.fit_transform(feat_sample)\n",
        "        scatter = ax3.scatter(z_emb[:, 0], z_emb[:, 1], c=y_sample, cmap='tab10', s=15, alpha=0.7)\n",
        "        ax3.legend(*scatter.legend_elements(), title=\"Classes\", loc=\"upper right\", ncol=2)\n",
        "        ax3.set_title(\"t-SNE Latent Space Visualization\", fontsize=14)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# PIPELINE GROUP 1: IDEC (Grayscale)\n",
        "def run_group1_idec(dataset_name, n_clusters, epochs_ae, epochs_idec, batch_size, device):\n",
        "    print(f\"\\n RUNNING GROUP 1 (IDEC): {dataset_name}\")\n",
        "\n",
        "    # Load Data (Force Clean Transform for AE)\n",
        "\n",
        "    train_loader, test_loader = get_data_loaders(dataset_name, batch_size, is_contrastive=False)\n",
        "\n",
        "    sample_batch, _ = next(iter(train_loader))\n",
        "    input_dim = sample_batch.view(sample_batch.size(0), -1).shape[1]\n",
        "    print(f\"   Input Dim: {input_dim}\")\n",
        "\n",
        "    # Model & Pre-train\n",
        "    ae = Autoencoder(input_dim=input_dim, latent_dim=10).to(device)\n",
        "    optimizer_ae = optim.Adam(ae.parameters(), lr=1e-3)\n",
        "\n",
        "    ae.train()\n",
        "    for _ in tqdm(range(epochs_ae), desc=\"   Step 1: Pre-train AE\", leave=False):\n",
        "        for x, _ in train_loader:\n",
        "            x = x.to(device)\n",
        "            if x.dim() > 2: x = x.view(x.size(0), -1)\n",
        "            x_recon, _ = ae(x)\n",
        "            loss = F.mse_loss(x_recon, x)\n",
        "            optimizer_ae.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer_ae.step()\n",
        "\n",
        "    # Init Centroids\n",
        "    print(\"   Step 2: Init K-Means...\")\n",
        "    ae.eval()\n",
        "    features = []\n",
        "    with torch.no_grad():\n",
        "        for x, _ in train_loader:\n",
        "            x = x.to(device)\n",
        "            if x.dim() > 2: x = x.view(x.size(0), -1)\n",
        "            _, z = ae(x)\n",
        "            features.append(z.cpu().numpy())\n",
        "    features = np.concatenate(features)\n",
        "    kmeans = KMeans(n_clusters=n_clusters, n_init=20, random_state=42).fit(features)\n",
        "\n",
        "    idec = IDEC(ae, n_clusters=n_clusters).to(device)\n",
        "    idec.clustering_layer.centroids.data = torch.tensor(kmeans.cluster_centers_).to(device)\n",
        "\n",
        "    # Train IDEC\n",
        "    optimizer_idec = optim.Adam(idec.parameters(), lr=1e-4)\n",
        "    idec.train()\n",
        "    for _ in tqdm(range(epochs_idec), desc=\"   Step 3: Fine-tune IDEC\", leave=False):\n",
        "        for x, _ in train_loader:\n",
        "            x = x.to(device)\n",
        "            x_recon, q, z = idec(x)\n",
        "            p = target_distribution(q)\n",
        "            loss, _, _ = idec_loss_function(x, x_recon, q, p, gamma=0.1)\n",
        "            optimizer_idec.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer_idec.step()\n",
        "\n",
        "    # Eval & Visualize\n",
        "    idec.eval()\n",
        "    final_z, final_pred, final_true = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x = x.to(device)\n",
        "            _, q, z = idec(x)\n",
        "            final_z.append(z.cpu().numpy())\n",
        "            final_pred.append(torch.argmax(q, dim=1).cpu().numpy())\n",
        "            final_true.append(y.numpy())\n",
        "\n",
        "    # Concat\n",
        "    final_z = np.concatenate(final_z)\n",
        "    final_pred = np.concatenate(final_pred)\n",
        "    final_true = np.concatenate(final_true)\n",
        "\n",
        "    results = evaluate_clustering(final_true, final_pred)\n",
        "    print(f\"   Result: {results}\")\n",
        "    Visualizer.plot_all(results, final_true, final_pred, final_z, dataset_name, \"IDEC\")\n",
        "\n",
        "# PIPELINE GROUP 2: SimCLR (Color/Complex)\n",
        "def run_group2_simclr(dataset_name, n_clusters, epochs, batch_size, device):\n",
        "    print(f\"\\nðŸ”¸ RUNNING GROUP 2 (SimCLR): {dataset_name}\")\n",
        "\n",
        "    # Load Data\n",
        "    # Loader train: is_contrastive=True (2 views)\n",
        "    train_loader, _ = get_data_loaders(dataset_name, batch_size, is_contrastive=True)\n",
        "    # Loader test: is_contrastive=False\n",
        "    _, test_loader = get_data_loaders(dataset_name, batch_size, is_contrastive=False)\n",
        "\n",
        "    # Model & Loss\n",
        "    model = ContrastiveResNet18(latent_dim=128).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
        "    criterion = NTXentLoss(batch_size=batch_size, temperature=0.5, device=device)\n",
        "\n",
        "    # Train SimCLR loop\n",
        "    model.train()\n",
        "    for epoch in tqdm(range(epochs), desc=\"   Step 1: Training SimCLR\", leave=False):\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            # batch[0] is list [view1, view2] because TwoCropTransform\n",
        "            (x_i, x_j), _ = batch\n",
        "            x_i, x_j = x_i.to(device), x_j.to(device)\n",
        "\n",
        "            _, z_i = model(x_i)\n",
        "            _, z_j = model(x_j)\n",
        "\n",
        "            loss = criterion(z_i, z_j)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    # Evaluation (K-Means on learned features h)\n",
        "    print(\"   Step 2: Evaluating Features...\")\n",
        "    model.eval()\n",
        "    features_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x = x.to(device)\n",
        "            h, _ = model(x)\n",
        "            features_list.append(h.cpu().numpy())\n",
        "            labels_list.append(y.numpy())\n",
        "\n",
        "    final_h = np.concatenate(features_list)\n",
        "    final_true = np.concatenate(labels_list)\n",
        "\n",
        "    # K-Means\n",
        "    kmeans = KMeans(n_clusters=n_clusters, n_init=20, random_state=42)\n",
        "    final_pred = kmeans.fit_predict(final_h)\n",
        "\n",
        "    results = evaluate_clustering(final_true, final_pred)\n",
        "    print(f\"    Result: {results}\")\n",
        "    Visualizer.plot_all(results, final_true, final_pred, final_h, dataset_name, \"SimCLR+KMeans\")\n",
        "\n",
        "# MASTER EXECUTION BLOCK\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    print(f\"System Check: Using {str(device).upper()}\")\n",
        "\n",
        "\n",
        "    all_datasets = [\n",
        "        # --- GROUP 1: IDEC ---\n",
        "        {'name': 'MNIST',          'group': 1, 'k': 10, 'ae': 30, 'idec': 30, 'bs': 256},\n",
        "        {'name': 'Fashion-MNIST',  'group': 1, 'k': 10, 'ae': 50, 'idec': 40, 'bs': 256},\n",
        "        {'name': 'USPS',           'group': 1, 'k': 10, 'ae': 50, 'idec': 30, 'bs': 256},\n",
        "        {'name': 'OPTDIGITS',      'group': 1, 'k': 10, 'ae': 50, 'idec': 30, 'bs': 64},\n",
        "\n",
        "        # --- GROUP 2: SimCLR ---\n",
        "        {'name': 'CIFAR-10',       'group': 2, 'k': 10, 'epochs': 50, 'bs': 128},\n",
        "        {'name': 'CIFAR-100-20',   'group': 2, 'k': 20, 'epochs': 50, 'bs': 128},\n",
        "        {'name': 'GTSRB',          'group': 2, 'k': 43, 'epochs': 50, 'bs': 64}\n",
        "    ]\n",
        "\n",
        "    for config in all_datasets:\n",
        "        try:\n",
        "            if config['group'] == 1:\n",
        "                run_group1_idec(\n",
        "                    dataset_name=config['name'],\n",
        "                    n_clusters=config['k'],\n",
        "                    epochs_ae=config['ae'],\n",
        "                    epochs_idec=config['idec'],\n",
        "                    batch_size=config['bs'],\n",
        "                    device=device\n",
        "                )\n",
        "            elif config['group'] == 2:\n",
        "                run_group2_simclr(\n",
        "                    dataset_name=config['name'],\n",
        "                    n_clusters=config['k'],\n",
        "                    epochs=config['epochs'],\n",
        "                    batch_size=config['bs'],\n",
        "                    device=device\n",
        "                )\n",
        "        except Exception as e:\n",
        "            print(f\" Error running {config['name']}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
